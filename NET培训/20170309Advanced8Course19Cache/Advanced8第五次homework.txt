增加一个别的网站的简单抓取任务



1  作业初级要求：根据使用说明使用crawler，完成数据抓取，如果有异常日志等，自己尝试解决，解决后将修改后的爬虫提交给老师，并加以描述哪里做了修改；

2  作业进阶要求：自行完成类别数据的抓取，别使用老师提供的代码；
 		 
	         换一种多线程任务分配的模式，完成商品数据的抓取；

                 试着将数据访问换成第一次作业的数据库访问层，尝试增加批量操作和事务操作；
                
                 将程序改写成三层结构的，dal  bll  winform  然后还有common  model；
		
 		 试试缓存(比如缓存一些反射生成的sql语句)

                 仔细阅读爬虫项目，提出改进意见，最好能自行完成尝试，并文档说明；

                 如果某个抓取出异常了，能够取消其他线程任务；


3  作业高阶要求：目前爬虫只能一次性完成，试试建立一个winform，提供

	         清理类别数据、清理商品数据、抓取类别数据、

                 商品数据抓取的开始、暂停、恢复等功能(没关掉程序)

                 (思考下如果程序关掉了，还要能继续抓取商品的话，要如何实现，这个写下思路即可)


4  作业终极要求：试试建立抓取一号店/淘宝/天猫/苏宁/国美 任意一家数据的爬虫，
                 要求包括商品的类别、商品数据表等结构，要求程序能正常抓取；

                 或者自行尝试别的数据抓取；

5  作业究极要求：解决下如何不进qq群，抓取qq群成员信息。。。。
                 现有的CrawlerQQ已经可以关键字抓取qq群信息和抓取有权限的群成员信息
                 (见压缩包里的CrawlerQQ)
                 这个是一个不可能完成的任务。。


1为必选任务，自行完成数据的抓取，这将是后面项目的数据源；
2 3 4任选一个完成，，作为进阶任务
5为拓展任务，超五星难度，如有完成，老师大红包奖励，长期有效



作业要求在下周二(03.14)晚上22点前发到邮箱：
           57265177@qq.com
并抄送到   25759541@qq.com
作业用压缩包的形式发给我，压缩包名称命名以群里的昵称开头，然后有更新的话，在昵称后面加上_update1这样子
第一次提交是：   24一两_homework1.rar
第二次提交是：   24一两_homework1_Update1.rar
第三次提交是：   24一两_homework1_Update2.rar

压缩包中希望包含作业说明文档，可以是完成过程中的问题、思路、解决方案、学习感悟、笔记都可以了，，
如果作业有更新，则必须把更新的内容写入文档，没有更新文档的拒绝批改(:


然后越早交作业的  老师会单独批改，一对一指导，超时交的作业可能没法即时批改。。。